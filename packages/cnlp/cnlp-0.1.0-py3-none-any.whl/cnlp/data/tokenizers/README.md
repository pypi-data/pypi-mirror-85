# 分词

## 1 简介

**词是能够独立活动的最小语言单位。**
在自然语言处理中，通常都是以词作为基本单位进行处理的。
由于英文本身具有天生的优势，以空格划分所有词。
而中文的词与词之间没有明显的分割标记，所以在做中文语言处理前的首要任务，就是把连续中文句子分割成「词序列」。
这个分割的过程就叫**分词**。

举个栗子🌰：

> 输入：词是能够独立活动的最小语言单位。
>
> 输出：词 / 是 / 能够 / 独立 / 活动 / 的 / 最小 / 语言 / 单位 / 。

## 2 难点

我们先来看看中文分词有那些难点，主要包括两个方面：「切分歧义识别」和「未登录词识别」。
下面👇用几个具体🌰感受一下：

### 1 切分歧义识别：

1.  交集型歧义：
    *   化妆 / **和服 / 装**
    *   化妆 / **和 / 服装**
2.  组合型歧义：
    *   这个 / 门 / **把手** / 坏了
    *   请 / **把 / 手** / 拿开
3.  真歧义：
    *   **乒乓 / 球拍 / 卖** / 完 / 了
    *   **乒乓球 / 拍卖** / 完 / 了

### 2 未登录词识别：

包括人名、机构名、地名、产品名、商标名、简称、省略语、新词等都是很难处理的问题。

举个🌰：「躺枪」、「洗摸杯」等。

## 3 技术分类

### 1 「基于字符匹配」 的分词方法；

又叫做「机械分词」方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。

1.  正向最大匹配法（由左到右的方向）；
2.  逆向最大匹配法（由右到左的方向）；
3.  最少切分（使每一句中切出的词数最小）；
4.  双向最大匹配法（进行由左到右、由右到左两次扫描）。

### 2 「基于词频统计」的分词方法；

一般来看，上下文中相邻的字出现在一起的次数越多，那么它们越有可能是一个词。因此词与词的共现频率可以反映出成词的可能性。这种方法可以很好的处理「新词」。常见的方法有 n-gram、t-test、HMM等（*这里先不做解释，后面会详细举例，挖坑*）。

### 3 「基于知识理解」的分词方法。

在分词的同时进行句法、语义的分析，利用句法信息和语义信息来处理歧义现象，因而具有良好的歧义切分能力，但因为要对语言自身信息进行更多的处理，因而加大了实现的难度。

## 3 实战

### 1 机械分词

机械分词是按照词典词语长短和词频对句子进行切分，优先切分出长的词语。

```python
from cnlp.data import Tokenizer

t = Tokenizer()

print(t.cut("为中华之崛起而读书", "mechanical"))
# ['为', '中华', '之', '崛起', '而', '读书']
```

### 2 基于图算法的分词

```python
from cnlp.data import Tokenizer

t = Tokenizer()

print(t.cut("为中华之崛起而读书", "dag"))
# ['为', '中华', '之', '崛起', '而', '读书']
```

## 4 公开数据集

*   [复旦大学 NLPCC2016 微博语料](https://github.com/FudanNLP/NLPCC-WordSeg-Weibo/tree/master/datasets)
*   [SIGHAN2005 新闻语料](http://sighan.cs.uchicago.edu/bakeoff2005/)

## 5 参考资料

*   [中文分词技术(中文分词原理)](http://www.cnblogs.com/flish/archive/2011/08/08/2131031.html)
*   [结巴中文分词](https://github.com/fxsjy/jieba)
*   [机器如何解读语言？中文分词算法你知道几种？](https://zhuanlan.zhihu.com/p/21440794)