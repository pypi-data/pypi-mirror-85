# [MSE/MAE][1]

> 二分类为啥不能用 mse loss: 非凸 局部最优解
## 解决方案
- 如果想要检测异常值则使用MSE，如果想学习一个预测模型则建议使用MAE，或者先进行异常值处理再使用MSE
- 数据先处理异常值: MSE
- 数据不处理异常值:
    - MAE
    - 损失函数替换：如 
        - msle: tf.losses.msle
        - huber loss: MAE + MSE
        ![](http://ww3.sinaimg.cn/large/006tNc79gy1g4mqft12jnj30lw04oglv.jpg)
        - quantile loss: 在数据方差不一致时表现最好的（MAE的拓展）http://www.neurta.com/node/448
        - Log-Cosh loss 比 l2 更光滑: tf.losses.logcosh
        优点：当x很小时，log(cosh(x))约等于(x^2)/2;当x很大时，他约等于abs(x)-log(2). logcosh几乎与MSE一样的效果，但是巧合出现的不正常预测对他的影响没有那么大。他具有Huber Loss 的所有优点，并且他处处二阶可导。
        Log-cosh也不是完美的。当预测值离目标很远时，他的梯度和Hessian矩阵也有问题。

    - 目标值变换
    - 模型融合
        - 头尾MSE
        - 中部MAE

两种损失函数的性质
MSE对异常值敏感，因为它的惩罚是平方的，所以异常值的loss会非常大。 
MAE对异常之不敏感（鲁棒），

不妨设拟合函数为常数，那么MSE就相当于所有数据的均值（列出loss对c求导即可），而MAE相当于所有数据的中位数，所以会对异常值不敏感。

优化效率
MAE不可导而且所有的导数的绝对值都相同，优化时无法确定更新速度， 
MSE可导，有closed-form解，只需要令偏导数为0即可。

我们认为MSE对于两端极值的预测较为准确,
而MAE则对于中间的预测更为准确,所以我们对函数预测的极值附近进行简单的加权修正。


最小化平方误差（MSE）绝对不是最小化误差的绝对偏差（MAD）。MSE提供的平均响应𝑦 条件的 𝑥，而MAD提供的中位响应𝑦 条件的 𝑥。

从历史上看，拉普拉斯最初将最大观测误差视为模型正确性的度量。他很快就开始考虑使用MAD了。由于他无法精确解决这两种情况，他很快就考虑了微分MSE。他自己和高斯（似乎同时）得出了正规方程，这是一个封闭形式的解决方案。如今，通过线性编程解决MAD相对容易。然而，众所周知，线性编程没有封闭形式的解决方案。

从优化角度来看，两者都对应于凸函数。然而，MSE是可微分的，因此，允许基于梯度的方法，比非不可区分的方法更有效。MAD是不可区分的𝑥 = 0。

进一步的理论原因是，在贝叶斯环境中，当假设模型参数的均匀先验时，MSE产生正态分布误差，这已被视为该方法正确性的证明。理论家喜欢正态分布，因为他们认为这是一个经验事实，而实验者喜欢它，因为他们认为这是一个理论结果。

MSE可能得到广泛接受的最后一个原因是它基于欧氏距离（事实上它是欧几里德空间投影问题的解决方案），考虑到我们的几何现实，这是非常直观的。
---
[1]: https://www.jianshu.com/p/1ff7ae7ea9ef






