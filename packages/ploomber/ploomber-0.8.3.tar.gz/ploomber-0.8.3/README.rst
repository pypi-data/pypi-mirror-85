Ploomber
========

.. image:: https://github.com/ploomber/ploomber/workflows/CI%20Linux/badge.svg
   :target: https://github.com/ploomber/ploomber/workflows/CI%20Linux/badge.svg
   :alt: CI Linux
  
.. image:: https://github.com/ploomber/ploomber/workflows/CI%20macOS/badge.svg
   :target: https://github.com/ploomber/ploomber/workflows/CI%20macOS/badge.svg
   :alt: CI macOS

.. image:: https://github.com/ploomber/ploomber/workflows/CI%20Windows/badge.svg
   :target: https://github.com/ploomber/ploomber/workflows/CI%20Windows/badge.svg
   :alt: CI Windows

.. image:: https://readthedocs.org/projects/ploomber/badge/?version=latest
    :target: https://ploomber.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

.. image:: https://mybinder.org/badge_logo.svg
 :target: https://mybinder.org/v2/gh/ploomber/projects/master

.. image:: https://badge.fury.io/py/ploomber.svg
  :target: https://badge.fury.io/py/ploomber

.. image:: https://coveralls.io/repos/github/ploomber/ploomber/badge.svg?branch=master
  :target: https://coveralls.io/github/ploomber/ploomber?branch=master


Coding an entire analysis pipeline in a single notebook file allows you to
develop your code interactively, but it creates an unmaintainable monolith that
easily breaks. Ploomber allows you to modularize your analysis in smaller
tasks without losing the power of an interactive notebook.

Imagine you have a pipeline that gets (``get.ipynb``), cleans (``clean.ipynb``)
and plots (``plot.ipynb``) data. All you  have to do to turn this into a data
pipeline is to declare a special cell at the top of your notebook with
dependencies and output files:

.. code-block:: python

    # top cell in clean.ipynb

    # get.ipynb must run before clean.ipynb
    upstream = ['get']
    # output files generated by clean.ipynb
    product = {'data': 'output/clean.csv'}


That's it! Execute ``ploomber build`` and your pipeline tasks will execute in
the right order.

Apart from Jupyter notebooks, Ploomber also supports (Python/R/SQL) scripts
and Python functions as pipeline tasks.

`Watch JupyterCon 2020 talk <https://www.youtube.com/watch?v=M6mtgPfsA3M>`_

Main features
-------------

1. **Jupyter integration**. When you open your notebooks, Ploomber will
automatically inject a new cell with the location of your input files, as
inferred from your ``upstream`` variable. If you open a Python or R script, it
will be converted to a notebook on the fly.

2. **Incremental builds**. Speed up execution by skipping tasks whose source
code hasn't changed.

3. **Pipeline testing**. Run tests upon task execution to verify that the output
data has the right properties (e.g. values within expected range).

4. **Pipeline inspection**. Start an interactive session with
``ploomber interact`` to debug your pipeline. Call
``dag['task_name'].debug()`` to start a debugging session.

5. **[Beta] Deployment to Kubernetes and Airflow**. You can develop and execute
locally. But if you want to scale things up, deploy
to `Kubernetes or Airflow <https://github.com/ploomber/soopervisor>`_

Try it out
----------

.. code-block:: shell

    # clone the sample projects
    git clone https://github.com/ploomber/projects

    # move to the machine learning pipeline example
    cd projects/ml-basic

    # install dependencies
    # 1) if you have conda installed
    conda env create -f environment.yml
    conda activate ml-basic
    # 2) if you don't have conda
    pip install ploomber pandas scikit-learn pyarrow sklearn-evaluation

    # create output folder
    mkdir output

    # run the pipeline
    ploomber build    


When execution finishes, you'll see the output in the ``output/`` folder.


Installation
------------

.. code-block:: shell

    pip install ploomber


Compatible with Python 3.6 and higher.


Resources
---------

* `Sample projects (Machine Learning pipeline, ETL, among others) <https://github.com/ploomber/projects>`_
* `Documentation <https://ploomber.readthedocs.io/>`_
* `Blog <https://ploomber.io/>`_
