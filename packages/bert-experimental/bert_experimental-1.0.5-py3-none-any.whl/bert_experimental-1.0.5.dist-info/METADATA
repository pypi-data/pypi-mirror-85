Metadata-Version: 2.1
Name: bert-experimental
Version: 1.0.5
Summary: Utilities for finetuning BERT-like models
Home-page: https://github.com/gaphex/bert_experimental
Author: Denis Antyukhov
Author-email: gaphex@gmail.com
License: MIT
Keywords: bert nlp tensorflow machine learning sentence encoding embedding finetuning
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3.7
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Description-Content-Type: text/markdown
Requires-Dist: tensorflow (<2.0,>=1.15)
Requires-Dist: tensorflow-hub (==0.7.0)
Requires-Dist: numpy

# bert_experimental

Contains code and supplementary materials for a series of Medium articles about the BERT model.

pretraining: https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379

feature_extraction: https://medium.com/@gaphex/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a

finetuning: https://medium.com/@gaphex/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2

representation: https://towardsdatascience.com/improving-sentence-embeddings-with-bert-and-representation-learning-dfba6b444f6b


