# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_adapters.repository.ipynb (unless otherwise specified).

__all__ = ['__all__', 'get_class', 'AbstractRepository', 'SparkRepository', 'PandasRepository']

# Cell
import abc
from ..domain import model
from collections import namedtuple
import pandas as pd
import importlib
from ysyfinance import config
from typing import List

# Cell
__all__=['AbstractRepository','SparkRepository','PandasRepository', 'get_class']

# Cell
def get_class(reference: str) -> model.AbstractModel:
        refarr = reference.split('.')
        class_name = refarr[-1]
        package_name = '.'.join(refarr[:-1])
        module = importlib.import_module(package_name)
        cls = getattr(module, class_name)
        if cls is None:
            raise Exception(f"Class {reference} doesn't exist")
        return cls


class AbstractRepository(abc.ABC):
    def __init__(self):
        self.seen = set() # type: Set[model.AbstractDataObject]
        self.session = None
        self.api = None
        self.data_path = None

    def _get_class(self, reference: str) -> model.AbstractModel:
        refarr = reference.split('.')
        class_name = refarr[-1]
        package_name = '.'.join(refarr[:-1])
        module = importlib.import_module(package_name)
        cls = getattr(module, class_name)
        if cls is None:
            raise Exception(f"Class {reference} doesn't exist")
        return cls

    def get(self, reference: str, pipeline_name: str = None) -> model.AbstractModel:
        cls = self._get_class(reference)
        dataobj_list = [ getattr(self, '_get_'+source.source_type)(source.source_table) \
                        for source in cls.pipeline._datasources ]

        if len(dataobj_list)==1:
            modelobj = cls( \
                reference=reference, \
                dataobject=dataobj_list[0],\
                api=self.api,\
                data_path=self.data_path \
            )
        else:
            modelobj = cls.from_dataobjects(*dataobj_list, \
                             reference=reference, \
                             api=self.api, \
                             data_path=self.data_path)
        if modelobj:
            self.seen.add(modelobj)
        return modelobj

    def load(self, *, reference: str, pipeline_name: str) -> model.AbstractModel:
        cls = self._get_class(reference)
        dest_table = cls.pipeline._desttables.get(pipeline_name).name
        dataobj = self._get_datalake(dest_table)
        modelobj = cls( \
            reference=reference, \
            dataobject=dataobj,\
            api=self.api,\
            data_path=self.data_path \
        )
        if modelobj:
            self.seen.add(modelobj)
        return modelobj

    @abc.abstractmethod
    def _get_sqlserver(self, reference: str) -> model.AbstractModel:
        raise NotImplementedError

    @abc.abstractmethod
    def _get_datalake(self, dest_table: str) -> model.AbstractModel:
        raise NotImplementedError


# Cell
from ..budget.domain import *
class SparkRepository(model.DeltaLakeMixin, AbstractRepository):
    def __init__(self, *, session, api, data_path=config.get_data_path()):
        super().__init__()
        self.session = session
        self.api = api
        self.data_path = data_path

    def _get_sqlserver(self, reference):
        dataframe=self.session.option("dbtable", reference) \
                    .load()
        return dataframe

    def _get_datalake(self, dest_table):
        return super().load_data(dest_table)

# Cell
class PandasRepository(AbstractRepository):
    def __init__(self, session):
        super().__init__()
        self.session = session

    def _get_datalake(self, reference, postproc_cb=None):
        json_content=self.session.load(reference)

        if postproc_cb is not None:
            json_content=postproc_cb(json_content)

        dataframe=pd.DataFrame.from_dict(json_content, orient='columns')
        return dataframe

    def _get_sqlserver(self, reference):
        pass