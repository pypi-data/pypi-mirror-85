# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_domain.model.ipynb (unless otherwise specified).


from __future__ import annotations


__all__ = ['logger', '__all__', 'HiveMixin', 'DeltaLakeMixin', 'DataSource', 'AdditionalData', 'DestTable',
           'PipelineCollection', 'ModelMeta', 'ModelAbcMeta', 'AbstractModel', 'PipelineFuncStruct', 'SparkModel',
           'PandasModel']

# Cell
#nbdev_comment from __future__ import annotations
import abc
import logging
from pyspark.sql import DataFrame
from functools import wraps, partial
from ysyfinance import commons
import copy
import pyspark.sql.functions as F
from ysyfinance import config
from pyspark.sql import SparkSession
from typing import Any, List
from collections import defaultdict, namedtuple
from operator import itemgetter
from dataclasses import dataclass
import IPython.core.debugger as db
from pathlib import Path
logger = logging.getLogger(__name__)

# Cell
__all__=['AbstractModel','PipelineCollection','SparkModel','HiveMixin','DeltaLakeMixin','PandasModel', \
        'ModelMeta', 'ModelAbcMeta','DataSource', 'DestTable', 'AdditionalData']

# Cell
class HiveMixin:
    def parse_dest_table(self, dest_table: str) -> (str, str) :
        #dest_table has the format: zone.database.table
        names = dest_table.split(".") \
                if dest_table is not None \
                and len(dest_table)>0 else None
        if names is None or names=='' or len(names)<3:
            raise Exception(f"Destination table {dest_table} has wrong name")
        zone_name=names[0]
        database_name=names[1]
        table_name=names[2]
        return (zone_name, database_name, table_name)

    def save(self, **kwargs):
        dest_table = kwargs.get('dest_table')
        mode = kwargs.get('mode') if kwargs.get('mode') is not None else 'overwrite'
        zone_name, database_name, table_name = self.parse_dest_table(dest_table)

        self._data.write \
                        .mode(mode) \
                        .save(f"{self.data_path}/{zone_name}/{database_name}/{table_name}")

    def saveAsTable(self, **kwargs):
        dest_table = kwargs.get('dest_table')
        mode = kwargs.get('mode') if kwargs.get('mode') is not None else 'overwrite'
        _, database_name, table_name = self.parse_dest_table(dest_table)

        self.sql(f"CREATE DATABASE IF NOT EXISTS {database_name}")
        self.sql(f"USE {database_name}")

        self._data.write \
                        .mode(mode) \
                        .saveAsTable(table_name)


# Cell
class DeltaLakeMixin(HiveMixin):
    def save(self, **kwargs):
        #db.set_trace()
        dest_table = kwargs.get('dest_table')
        mode = kwargs.get('mode') if kwargs.get('mode') is not None else 'overwrite'
        zone_name, database_name, table_name = self.parse_dest_table(dest_table)

        self._dataobject.write \
                        .format('delta') \
                        .mode(mode) \
                        .option("mergeSchema", "true") \
                        .option("overwriteSchema", "true") \
                        .save(f"{self.data_path}/{zone_name}/{database_name}/{table_name}")

    def upsert(self, **kwargs):
#         db.set_trace()
        from delta.tables import DeltaTable
        dest_table = kwargs.get('dest_table', None)
        primary_key = kwargs.get('primary_key', None)

        zone_name, database_name, table_name = self.parse_dest_table(dest_table)
        dest_table_file = f"{self.data_path}/{zone_name}/{database_name}/{table_name}"

        #db.set_trace()
        if not Path(dest_table_file).is_dir():
            super(SparkModel, self).save(dest_table=dest_table)
            return

        deltaTable = DeltaTable.forPath(self.api, dest_table_file)
        merge_expr = ' AND '.join(f"dest.{key} = source.{key}" for key in primary_key)
        deltaTable.alias("dest") \
                  .merge(self._dataobject.alias("source"), merge_expr) \
                  .whenMatchedUpdateAll() \
                  .whenNotMatchedInsertAll() \
                  .execute()

    def saveAsTable(self, **kwargs):
        dest_table = kwargs.get('dest_table')
        mode = kwargs.get('mode') if kwargs.get('mode') is not None else 'overwrite'
        _, database_name, table_name = self.parse_dest_table(dest_table)


        self._dataobject.sql(f"CREATE DATABASE IF NOT EXISTS {database_name}")
        self._dataobject.sql(f"USE {database_name}")

        self._dataobject.write \
                        .format('delta') \
                        .mode(mode) \
                        .saveAsTable(table_name)

    def load_data(self, dest_table: str):
        zone_name, database_name, table_name = self.parse_dest_table(dest_table)

        return self.api.read \
                .format('delta') \
                .load(f"{self.data_path}/{zone_name}/{database_name}/{table_name}")


# Cell
@dataclass
class DataSource:
    #sqlserver or datalake
    source_type: str = 'sqlserver'
    #format: database.table
    source_table: str = None

@dataclass
class AdditionalData:
    #stored or pipeline
    data_type: str = 'stored'
    reference: str = None

@dataclass
class DestTable:
    name: str
    primary_key: List[str] = None

class PipelineCollection():
    """
        Stores the funcs which will be called in AbstractModel run_pipeline

        Args:

            _funcs: a dict to store the transformation function for a pipeline
                    for example: {'pipeline_a': {'func_a': actual_func} }
            _desttables: a dict to store the destination location for each pipeline
            _datasources: a list of DataSource object
            _additional_data: a dict to store AdditionalData for each pipeline
    """
    def __init__(self):
        self._funcs=defaultdict(dict)
        self._desttables={} # type: Dict[str, str]
        self._datasources=[] # type: List[DataSource]
        self._additional_data={} # type: Dict[str, List[AdditionalData]]

    def register(self, *, pipeline_name: str = None, order : int = 0):
        '''
            Decorator to register a transformation function to a pipeline.
            It will be called sequentially based on its order."
        '''
        if pipeline_name is None: raise Exception('Please provide pipleline name.')
        def decorate(func):
            func.order = order
            self._funcs[pipeline_name][func.__name__] = func
#             self._funcs[func.__name__]=func
            @wraps(func)
            def wrapper(*args, **kwargs):
                return func(*args, **kwargs)
            return wrapper
        return decorate

    def add_desttable(self, *, pipeline_name: str = None, desttable: str = None, primary_key: List[str] = None):
        if pipeline_name is None or desttable is None:
            raise Exception('Please provide pipeline name or desttable name')
        dest = DestTable(name=desttable, primary_key=primary_key)
        self._desttables[pipeline_name]=dest

    def get_pipeline_funcs(self, pipeline_name: str):
        return self._funcs.get(pipeline_name)

    def add_source(self, source: DataSource):
        self._datasources.append(source)

    def add_additional_data(self, pipeline_name: str = None, additional_data = None):
        self._additional_data[pipeline_name] = additional_data

# Cell
class ModelMeta(type):
    "Meta class to add class attribute pipeline to all child classes of AbstractModel"
    @classmethod
    def __prepare__(cls, clsname, bases):
        return {'pipeline': PipelineCollection()}

class ModelAbcMeta(ModelMeta, abc.ABCMeta):
    pass

# Cell
PipelineFuncStruct = namedtuple('PipelineFuncStruct',['name','order','func'])

class AbstractModel(abc.ABC, metaclass=ModelAbcMeta):
#     pipeline = PipelineCollection()
    source_table = None
#     dest_table = None

    def __init__(self, *, reference=None, dataobject=None, api=None, data_path=None):
        self.reference = reference
        self._dataobject = dataobject
        self.api = api
        self.data_path = data_path
        self._pipeline_dataobjects = {}
        self.events = [] # type: List[events.Event]

    @classmethod
    def get_pipeline_names(cls):
        return list(cls.pipeline._funcs)

    @property
    def dataframe(self):
        return self._dataobject

    def __getattr__(self, name):
        if self._dataobject is None:
            raise Exception(f"{name} not implemented")

        return getattr(self._dataobject,name)

    def _copy(self):
        model=type(self).__new__(type(self))
        model.api=self.api
        model.data_path=self.data_path
        model.reference=self.reference
        model.events=copy.copy(self.events)
        return model

    def get_chained_pipelines(self, pipeline_name: str):
        chained_funcs = copy.copy(self.pipeline.get_pipeline_funcs(pipeline_name))
        for cls in type(self).__mro__:
            pipeline_collection = cls.__dict__.get('pipeline')
            if pipeline_collection is not None \
                and pipeline_collection.get_pipeline_funcs(pipeline_name) is not None \
                and isinstance(pipeline_collection, PipelineCollection):
                pipeline_funcs = pipeline_collection.get_pipeline_funcs(pipeline_name).copy()
                pipeline_funcs.update(chained_funcs)
                chained_funcs = pipeline_funcs
        return chained_funcs

    def sort_chained_pipelines(self, *, chained_funcs, order_key = 'order'):
        key = lambda o: getattr(o, order_key, 0)
        x = self._dataobject
        list_funcs = []
        for k,v in chained_funcs.items():
#             list_funcs.append({'name': v.__qualname__, '_order': v._order, 'func': v})
            list_funcs.append(PipelineFuncStruct(
                                name=v.__qualname__,
                                order=v.order,
                                func=v
                                )
                             )
        sorted_funcs = sorted(list_funcs, key=key)
        return sorted_funcs

    def print_pipeline(self, pipeline_name: str, indent='    '):
#         db.set_trace()
        chained_funcs = self.get_chained_pipelines(pipeline_name)
        sorted_funcs = self.sort_chained_pipelines(chained_funcs=chained_funcs)

        if len(sorted_funcs)<1:
            print("Pipeline name={pipeline_name} is empty")
            return
        pipeline_str = "{}Pipeline name = {}\n".format(indent, pipeline_name)
        #db.set_trace()
        if self.pipeline._additional_data.get(pipeline_name, None):
            pipeline_str += "{}Start from {!r} => ".format(indent*2, self.pipeline._additional_data.get(pipeline_name, None))
        pipeline_str += "Functions = {}(order={})" \
                            .format(
                                    sorted_funcs[0].name, \
                                    sorted_funcs[0].order \
                                   )
        for i, func in enumerate(sorted_funcs):
            if i>=1:
                pipeline_str+=" -> {}(order={})" \
                                .format(func.name, func.order)

        print(pipeline_str)

    def _load(self, *, pipeline_name : str = None):
        "loads the stored data by a pipeline"
        raise NotImplementedError

    def _get(self, *, pipeline_name: str = None):
        "gets the ouput dataobject of a pipeline"
        return self._pipeline_dataobjects.get(pipeline_name, None)

    def run_pipeline(self, *args, pipeline_name :str = None, order_key: str = 'order', **kwargs):
        '''traverse through the class hierarchy and
        call func in _funcs sequentially, return the result as a new AbstractModel object'''
#         db.set_trace()
        if pipeline_name is None: raise Exception('Please provide pipeline name.')
        if self.pipeline._funcs.get(pipeline_name) is None:
            raise Exception("Pipeline {pipeline_name} doesn't exist")

        chained_funcs = self.get_chained_pipelines(pipeline_name)
        sorted_funcs = self.sort_chained_pipelines(chained_funcs=chained_funcs, order_key=order_key)

        x = self._dataobject
        pipeline_dataobjects = None

        if self.pipeline._additional_data.get(pipeline_name, None):
            pipeline_dataobjects = []
            for a_data in self.pipeline._additional_data.get(pipeline_name, None):
                if a_data.data_type == 'stored':
#                     db.set_trace()
                    o = self._load(pipeline_name=a_data.reference)
                elif a_data.data_type == 'pipeline':
                    o = self._get(a_data.reference)
#                     o = self._pipeline_dataobjects.get(a_data.reference, None)
                else:
                    raise Exception(f'{a_data.type} undefined')
                #db.set_trace()
                if not o:
                    raise Exception(f'Missing additional data {a_data.reference} data object.')
                pipeline_dataobjects.append(o)


        for f in sorted_funcs:
            if not pipeline_dataobjects:
                x = f.func(self, x, *args, **kwargs)
            else:
                x = f.func(self, x, *pipeline_dataobjects, *args, **kwargs)

        model = self._copy()
        model._dataobject = x
        self._pipeline_dataobjects[pipeline_name] = x
        return model

    def __repr__(self):
        class_name=type(self).__name__
        return '{0}({1._dataobject!r}, {1.pipeline!r})'.format(class_name, self)

    def __str__(self):
        return '({0._dataobject!s},{0.pipeline!s})'.format(self)


    def set_func_order(self, *, pipeline_name: str, func_name : str, order : int = 0 ):
        "change the order of the function in the _funcs dict for the object"
        func = self.pipeline._funcs[pipeline_name].get(func_name)
        if func is None:
            return
        logger.debug('set_func_order, old order is %s', func.order)
        func.__dict__['order'] = order
        logger.debug('set_func_order, new order is %s', func.order)

    def get_func_order(self, *, pipeline_name: str, func_name):
        func = self.pipeline._funcs[pipeline_name].get(func_name)
        return func.order

    @classmethod
    def from_dataobjects(cls, *args, reference:str, api:Any, data_path:str) -> AbstractModel:
        dataobject = cls._combine_dataobjects(*args)
        modelobj = cls( \
                reference=reference, \
                dataobject=dataobject,\
                api=api,\
                data_path=data_path
            )

        return modelobj

    @classmethod
    def _combine_dataobjects(cls, *args) -> Any:
        raise NotImplementedError

# Cell
class SparkModel(DeltaLakeMixin, AbstractModel):
    #format: database.table
#     source_table = None
    #format: zone.database.table
#     dest_table = None
#     pipeline = PipelineCollection()

    def save(self, *, pipeline_name, mode :str ='overwrite'):
        #db.set_trace()
        dest_table = self.pipeline._desttables.get(pipeline_name, None)
        if not dest_table:
            return
        if not dest_table.primary_key:
            super().save(dest_table=dest_table.name, mode=mode)
        else:
            super().upsert(dest_table=dest_table.name, primary_key=dest_table.primary_key)

    def saveAsTable(self, *, pipeline_name, mode='overwrite'):
        dest_table = self.pipeline._desttables[pipeline_name]
        super().saveAsTable(dest_table=dest_table, mode=mode)

    def load(self, *, pipeline_name):
        dest_table = self.pipeline._desttables.get(pipeline_name, None)
        if not dest_table:
            raise Exception(f'Data {dest_table.name} does not exist')
        return super().load_data(dest_table=dest_table.name)

    def _load(self, *, pipeline_name):
        return self.load(pipeline_name=pipeline_name)

# Cell
class PandasModel(AbstractModel):
    pass