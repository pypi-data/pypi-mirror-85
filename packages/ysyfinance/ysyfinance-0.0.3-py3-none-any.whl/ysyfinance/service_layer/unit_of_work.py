# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/05_service_layer.unit_of_work.ipynb (unless otherwise specified).


from __future__ import annotations


__all__ = ['__all__', 'AbstractUnitOfWork', 'spark_sessionmaker', 'DEFAULT_SPARK_SESSION_FACTORY',
           'pandas_sessionmaker', 'DEFAULT_PD_SESSION_FACTORY', 'SparkUnitOfWork', 'PandasUnitOfWork', 'get_uow']

# Cell
#nbdev_comment from __future__ import annotations
import abc
from ysyfinance import config
from ..adapters import repository
from ..adapters import cache
from ..domain import model
import weakref
from pyspark.sql import SparkSession
from ysyfinance import commons
from pathlib import Path

# Cell
__all__=['AbstractUnitOfWork','spark_sessionmaker','SparkUnitOfWork',\
         'PandasUnitOfWork','pandas_sessionmaker','get_uow']

# Cell
class AbstractUnitOfWork(abc.ABC):
    repo: repository.AbstractRepository = None

    def __enter__(self) -> AbstractUnitOfWork:
        return self

    def __exit__(self, *args):
        self.rollback()

    def commit(self):
        self._commit()

    def collect_new_events(self):
        if self.repo is not None:
            for modelobj in self.repo.seen:
                while modelobj.events:
                    yield modelobj.events.pop(0)

    @abc.abstractmethod
    def _commit(self):
        raise NotImplementedError

    @abc.abstractmethod
    def rollback(self):
        raise NotImplementedError

    def _create_api(self):
        return None

# Cell
_spark_session_cache=weakref.WeakValueDictionary()

def spark_sessionmaker(
                 session_name='syndb', \
                 url=config.get_syndb_uri(), \
                 user=config.get_syndb_credential()[0], \
                 password=config.get_syndb_credential()[1]):
    def get_session(spark):
        if session_name not in _spark_session_cache:
            s = spark.read.format("jdbc") \
                .option("url", url) \
                .option("user", user) \
                .option("password", password) \
                .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")
            _spark_session_cache[session_name]=s
        else:
            s = _spark_session_cache[session_name]
        return s
    return get_session

# Cell
DEFAULT_SPARK_SESSION_FACTORY = spark_sessionmaker()

# Cell
def pandas_sessionmaker(path=Path(config.get_data_path())):
    def get_session():
        return cache.PickleCache(path)
    return get_session

# Cell
DEFAULT_PD_SESSION_FACTORY = pandas_sessionmaker()

# Cell
class SparkUnitOfWork(AbstractUnitOfWork):
    def __init__(self, session_factory=DEFAULT_SPARK_SESSION_FACTORY, data_path=config.get_data_path()):
        self.session_factory = session_factory
        self.data_path = data_path

    def _create_api(self) -> SparkSession:
        from pyspark.sql import SparkSession
        import os
        from os.path import join, abspath
        PACKAGES="io.delta:delta-core_2.12:0.7.0"
        os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages {PACKAGES} pyspark-shell'

        # use below code for hive database and table
        #     warehouse_location = abspath('spark-warehouse')
        #     print(f"warehouse location: {warehouse_location}")

        return SparkSession.builder \
                    .master('local') \
                    .appName("YSYFinance") \
                    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
                    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
                    .getOrCreate()

    def __enter__(self):
        api = self._create_api()
        self.session = self.session_factory(api)  # type:  pyspark.sql.DataFrameReader
        self.repo = repository.SparkRepository(session=self.session, \
                                               api=api, \
                                               data_path=self.data_path)
        return super().__enter__()

    def __exit__(self, *args):
        super().__exit__(*args)

    def _commit(self):
        pass

    def rollback(self):
        pass


# Cell
class PandasUnitOfWork(AbstractUnitOfWork):
    def __init__(self, session_factory=DEFAULT_PD_SESSION_FACTORY):
        self.session_factory = session_factory

    def __enter__(self):
        self.session = self.session_factory()  # type:  pyspark.sql.DataFrameReader
        self.repo = repository.PandasRepository(self.session)
        return super().__enter__()

    def __exit__(self, *args):
        super().__exit__(*args)

    def _commit(self):
        pass

    def rollback(self):
        pass

# Cell
def get_uow(spark_sessionmaker, data_path=config.get_data_path()) -> (SparkSession, SparkUnitOfWork):
    # use below to debug
#     db.set_trace()
    spark_session_factory=spark_sessionmaker()
    uow=SparkUnitOfWork(spark_session_factory, data_path)
    return uow